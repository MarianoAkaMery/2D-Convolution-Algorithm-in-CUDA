{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oDfyrVUoi9p0",
        "aO9tJ69Qo7uM",
        "xqxuVMTxpRtq",
        "IXgu6mWGz7Bg"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nNS2FA5BR0bU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3f77cb0-8209-4d5a-8d9a-601959e0ed58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize a nvcc plugin for python notebook"
      ],
      "metadata": {
        "id": "n6hCYQF3T2f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvcc4jupyter"
      ],
      "metadata": {
        "id": "e1MqBxDxUBTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb415c95-2a76-4539-a9a2-c0371ee09688"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the plugin extension"
      ],
      "metadata": {
        "id": "dlUTHXk-UM0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "id": "V23O5ZJFUQn4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0daaa0d1-b28f-48cd-a21a-fb10073844a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The nvcc4jupyter extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc4jupyter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parallel Matrix multiplication version 1"
      ],
      "metadata": {
        "id": "CNqvHWAkUi0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "#define PRINT_LOG false // Toggle for logging matrices and intermediate outputs\n",
        "\n",
        "// Kernel for 2D convolution using shared memory without tiling\n",
        "__global__ void convolution_2D_shared_no_tiling(unsigned char *in, unsigned char *mask, unsigned char *out, int maskwidth, int w, int h) {\n",
        "    // Declare dynamically allocated shared memory\n",
        "    extern __shared__ unsigned char shared_mem[];\n",
        "    unsigned char *shared_in = shared_mem; // Shared memory for input\n",
        "    unsigned char *shared_mask = shared_mem + (blockDim.y + maskwidth - 1) * (blockDim.x + maskwidth - 1); // Shared memory for kernel/mask\n",
        "\n",
        "    int tx = threadIdx.x; // Thread index in block (x-dimension)\n",
        "    int ty = threadIdx.y; // Thread index in block (y-dimension)\n",
        "    int Row = blockIdx.y * blockDim.y + ty; // Global row index\n",
        "    int Col = blockIdx.x * blockDim.x + tx; // Global column index\n",
        "\n",
        "    // Load kernel/mask into shared memory\n",
        "    if (ty < maskwidth && tx < maskwidth) {\n",
        "        shared_mask[ty * maskwidth + tx] = mask[ty * maskwidth + tx];\n",
        "    }\n",
        "\n",
        "    // Load the input matrix into shared memory with boundary checks\n",
        "    int input_row = Row - maskwidth / 2;\n",
        "    int input_col = Col - maskwidth / 2;\n",
        "\n",
        "    if (input_row >= 0 && input_row < h && input_col >= 0 && input_col < w) {\n",
        "        shared_in[ty * (blockDim.x + maskwidth - 1) + tx] = in[input_row * w + input_col];\n",
        "    } else {\n",
        "        shared_in[ty * (blockDim.x + maskwidth - 1) + tx] = 0; // Handle out-of-bounds access\n",
        "    }\n",
        "\n",
        "    __syncthreads(); // Ensure all threads have loaded data into shared memory\n",
        "\n",
        "    // Perform convolution\n",
        "    int pixVal = 0; // Accumulator for pixel value\n",
        "    if (Row < h && Col < w) { // Check boundaries for valid computation\n",
        "        for (int j = 0; j < maskwidth; ++j) {\n",
        "            for (int k = 0; k < maskwidth; ++k) {\n",
        "                pixVal += shared_in[(ty + j) * (blockDim.x + maskwidth - 1) + (tx + k)] * shared_mask[j * maskwidth + k];\n",
        "            }\n",
        "        }\n",
        "        out[Row * w + Col] = (unsigned char)(pixVal); // Store result in output matrix\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for 2D convolution using shared memory with tiling\n",
        "__global__ void convolution_2D_shared_with_tiling(unsigned char *in, unsigned char *mask, unsigned char *out, int maskwidth, int w, int h) {\n",
        "    extern __shared__ unsigned char shared_mem[];\n",
        "    unsigned char *tile = shared_mem; // Shared memory for tile\n",
        "    unsigned char *shared_mask = shared_mem + (blockDim.y + maskwidth - 1) * (blockDim.x + maskwidth - 1); // Shared memory for kernel/mask\n",
        "\n",
        "    int tx = threadIdx.x; // Thread index in block (x-dimension)\n",
        "    int ty = threadIdx.y; // Thread index in block (y-dimension)\n",
        "    int Row = blockIdx.y * blockDim.y + ty; // Global row index\n",
        "    int Col = blockIdx.x * blockDim.x + tx; // Global column index\n",
        "\n",
        "    // Load kernel/mask into shared memory\n",
        "    if (ty < maskwidth && tx < maskwidth) {\n",
        "        shared_mask[ty * maskwidth + tx] = mask[ty * maskwidth + tx];\n",
        "    }\n",
        "\n",
        "    // Load the input tile into shared memory\n",
        "    int input_row = Row - maskwidth / 2;\n",
        "    int input_col = Col - maskwidth / 2;\n",
        "\n",
        "    if (input_row >= 0 && input_row < h && input_col >= 0 && input_col < w) {\n",
        "        tile[ty * (blockDim.x + maskwidth - 1) + tx] = in[input_row * w + input_col];\n",
        "    } else {\n",
        "        tile[ty * (blockDim.x + maskwidth - 1) + tx] = 0; // Handle out-of-bounds access\n",
        "    }\n",
        "\n",
        "    __syncthreads(); // Synchronize threads to ensure the tile is fully loaded\n",
        "\n",
        "    // Perform convolution\n",
        "    int pixVal = 0; // Accumulator for pixel value\n",
        "    if (Row < h && Col < w) { // Check boundaries for valid computation\n",
        "        for (int j = 0; j < maskwidth; ++j) {\n",
        "            for (int k = 0; k < maskwidth; ++k) {\n",
        "                pixVal += tile[(ty + j) * (blockDim.x + maskwidth - 1) + (tx + k)] * shared_mask[j * maskwidth + k];\n",
        "            }\n",
        "        }\n",
        "        out[Row * w + Col] = (unsigned char)(pixVal); // Store result in output matrix\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Variables for GPU properties\n",
        "    int block_size;\n",
        "\n",
        "    // Retrieve information about available CUDA devices\n",
        "    int nDevices;\n",
        "    cudaGetDeviceCount(&nDevices);\n",
        "    for (int i = 0; i < nDevices; i++) {\n",
        "        cudaDeviceProp prop;\n",
        "        cudaGetDeviceProperties(&prop, i);\n",
        "        printf(\"Device Number: %d\\n\", i);\n",
        "        printf(\"  Device name: %s\\n\", prop.name);\n",
        "        printf(\"  max Blocks Per MultiProcessor: %d\\n\", prop.maxBlocksPerMultiProcessor);\n",
        "        printf(\"  max Threads Per MultiProcessor: %d\\n\", prop.maxThreadsPerMultiProcessor);\n",
        "        printf(\"  max Threads Per Block: %d\\n\", prop.maxThreadsPerBlock);\n",
        "        printf(\"  num SM: %d\\n\", prop.multiProcessorCount);\n",
        "        printf(\"  num bytes sharedMem Per Block: %d\\n\", prop.sharedMemPerBlock);\n",
        "        printf(\"  num bytes sharedMem Per Multiprocessor: %d\\n\", prop.sharedMemPerMultiprocessor);\n",
        "        printf(\"  Memory Clock Rate (KHz): %d\\n\", prop.memoryClockRate);\n",
        "        printf(\"  Memory Bus Width (bits): %d\\n\", prop.memoryBusWidth);\n",
        "        printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\", 2.0 * prop.memoryClockRate * (prop.memoryBusWidth / 8) / 1.0e6);\n",
        "    }\n",
        "\n",
        "    // Define matrix and kernel sizes\n",
        "    int matrix_sizes[] = {32, 128, 512, 1024, 2048, 4096, 8192, 10000};\n",
        "    int mask_sizes[] = {3, 7, 15, 31, 63, 127};\n",
        "\n",
        "    // Loop over matrix and kernel sizes\n",
        "    for (int m_idx = 0; m_idx < sizeof(matrix_sizes) / sizeof(matrix_sizes[0]); ++m_idx) {\n",
        "        for (int k_idx = 0; k_idx < sizeof(mask_sizes) / sizeof(mask_sizes[0]); ++k_idx) {\n",
        "            int MATRIX_SIZE = matrix_sizes[m_idx];\n",
        "            int MASK_WIDTH = mask_sizes[k_idx];\n",
        "            int MATRIX_WITH, MATRIX_HEIGHT = matrix_sizes[m_idx];\n",
        "\n",
        "            if (MASK_WIDTH >= MATRIX_SIZE) continue; // Skip cases where the kernel is larger than the matrix\n",
        "\n",
        "            printf(\"\\nRunning for MATRIX_SIZE = %d and MASK_WIDTH = %d\\n\", MATRIX_SIZE, MASK_WIDTH);\n",
        "\n",
        "            // Allocate memory for host data\n",
        "            unsigned char *a, *h_out, *h_mask;\n",
        "            a = (unsigned char *)malloc(sizeof(unsigned char) * MATRIX_SIZE * MATRIX_SIZE);\n",
        "            h_out = (unsigned char *)malloc(sizeof(unsigned char) * MATRIX_SIZE * MATRIX_SIZE);\n",
        "            h_mask = (unsigned char *)malloc(sizeof(unsigned char) * MASK_WIDTH * MASK_WIDTH);\n",
        "\n",
        "            if (PRINT_LOG) {\n",
        "                // Print the kernel/mask\n",
        "                printf(\"\\nMask:  \\n\");\n",
        "                for (int i = 0; i < MASK_WIDTH; ++i) {\n",
        "                    printf(\"[\");\n",
        "                    for (int j = 0; j < MASK_WIDTH; ++j) {\n",
        "                        h_mask[i * MASK_WIDTH + j] = 2;\n",
        "                        printf(\" %d \", h_mask[i * MASK_WIDTH + j]);\n",
        "                    }\n",
        "                    printf(\"]\\n\");\n",
        "                }\n",
        "            }\n",
        "\n",
        "            if (PRINT_LOG) {\n",
        "                // Print the input matrix\n",
        "                printf(\"\\nMatrix:  \\n\");\n",
        "                for (int i = 0; i < MATRIX_SIZE; ++i) {\n",
        "                    printf(\"[\");\n",
        "                    for (int j = 0; j < MATRIX_SIZE; ++j) {\n",
        "                        a[i * MATRIX_SIZE + j] = 1;\n",
        "                        printf(\" %d \", a[i * MATRIX_SIZE + j]);\n",
        "                    }\n",
        "                    printf(\"]\\n\");\n",
        "                }\n",
        "            }\n",
        "\n",
        "            // Allocate memory on the GPU\n",
        "            unsigned char *d_in, *d_mask, *d_out;\n",
        "            cudaMalloc((void **)&d_in, sizeof(unsigned char) * MATRIX_SIZE * MATRIX_SIZE);\n",
        "            cudaMalloc((void **)&d_mask, sizeof(unsigned char) * MASK_WIDTH * MASK_WIDTH);\n",
        "            cudaMalloc((void **)&d_out, sizeof(unsigned char) * MATRIX_SIZE * MATRIX_SIZE);\n",
        "\n",
        "            // Copy input data to device\n",
        "            cudaMemcpy(d_in, a, sizeof(unsigned char) * MATRIX_SIZE * MATRIX_SIZE, cudaMemcpyHostToDevice);\n",
        "            cudaMemcpy(d_mask, h_mask, sizeof(unsigned char) * MASK_WIDTH * MASK_WIDTH, cudaMemcpyHostToDevice);\n",
        "\n",
        "            // Configure thread and block dimensions\n",
        "            int block_x = 16;\n",
        "            int block_y = 16;\n",
        "            dim3 dimBlock(block_x, block_y);\n",
        "            dim3 dimGrid((MATRIX_SIZE + block_x - 1) / block_x, (MATRIX_SIZE + block_y - 1) / block_y, 1);\n",
        "\n",
        "            // Measure elapsed time for the no-tiling kernel\n",
        "            float elapsed_time_ms;\n",
        "            cudaEvent_t start, stop;\n",
        "            cudaEventCreate(&start);\n",
        "            cudaEventCreate(&stop);\n",
        "\n",
        "            cudaEventRecord(start, 0);\n",
        "            convolution_2D_shared_no_tiling<<<dimGrid, dimBlock, (block_x + MASK_WIDTH - 1) * (block_y + MASK_WIDTH - 1) + MASK_WIDTH * MASK_WIDTH>>>(d_in, d_mask, d_out, MASK_WIDTH, MATRIX_SIZE, MATRIX_SIZE);\n",
        "            cudaDeviceSynchronize();\n",
        "            cudaEventRecord(stop, 0);\n",
        "            cudaEventSynchronize(stop);\n",
        "            cudaMemcpy(h_out, d_out, sizeof(unsigned char) * MATRIX_SIZE * MATRIX_SIZE, cudaMemcpyDeviceToHost);\n",
        "\n",
        "            if (PRINT_LOG) {\n",
        "                // Print the output matrix for no-tiling\n",
        "                printf(\"\\nOutput Matrix (No Tiling):\\n\");\n",
        "                for (int i = 0; i < MATRIX_SIZE; ++i) {\n",
        "                    printf(\"[\");\n",
        "                    for (int j = 0; j < MATRIX_SIZE; ++j) {\n",
        "                        printf(\" %d \", h_out[i * MATRIX_SIZE + j]);\n",
        "                    }\n",
        "                    printf(\"]\\n\");\n",
        "                }\n",
        "            }\n",
        "\n",
        "            cudaEventElapsedTime(&elapsed_time_ms, start, stop);\n",
        "            printf(\"\\nconvolution_2D_shared_no_tiling Time elapsed: %f ms\\n\", elapsed_time_ms);\n",
        "\n",
        "            // Measure elapsed time for the tiling kernel\n",
        "            float elapsed_time_tiling_ms;\n",
        "            cudaEvent_t start_tiling, stop_tiling;\n",
        "            cudaEventCreate(&start_tiling);\n",
        "            cudaEventCreate(&stop_tiling);\n",
        "\n",
        "            cudaEventRecord(start_tiling, 0);\n",
        "            convolution_2D_shared_with_tiling<<<dimGrid, dimBlock, (block_x + MASK_WIDTH - 1) * (block_y + MASK_WIDTH - 1) + MASK_WIDTH * MASK_WIDTH>>>(d_in, d_mask, d_out, MASK_WIDTH, MATRIX_SIZE, MATRIX_SIZE);\n",
        "            cudaDeviceSynchronize();\n",
        "            cudaEventRecord(stop_tiling, 0);\n",
        "            cudaEventSynchronize(stop_tiling);\n",
        "            cudaMemcpy(h_out, d_out, sizeof(unsigned char) * MATRIX_SIZE * MATRIX_SIZE, cudaMemcpyDeviceToHost);\n",
        "\n",
        "            if (PRINT_LOG) {\n",
        "                // Print the output matrix for tiling\n",
        "                printf(\"\\nOutput Matrix (With Tiling):\\n\");\n",
        "                for (int i = 0; i < MATRIX_SIZE; ++i) {\n",
        "                    printf(\"[\");\n",
        "                    for (int j = 0; j < MATRIX_SIZE; ++j) {\n",
        "                        printf(\" %d \", h_out[i * MATRIX_SIZE + j]);\n",
        "                    }\n",
        "                    printf(\"]\\n\");\n",
        "                }\n",
        "            }\n",
        "\n",
        "            cudaEventElapsedTime(&elapsed_time_tiling_ms, start_tiling, stop_tiling);\n",
        "            printf(\"\\nconvolution_2D_shared_with_tiling Time elapsed: %f ms\\n\", elapsed_time_tiling_ms);\n",
        "\n",
        "            // Free GPU memory\n",
        "            cudaFree(d_in);\n",
        "            cudaFree(d_mask);\n",
        "            cudaFree(d_out);\n",
        "            // Free host memory\n",
        "            free(a);\n",
        "            free(h_out);\n",
        "            free(h_mask);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return 0; // Exit the program\n",
        "}"
      ],
      "metadata": {
        "id": "patSEnDlUqdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfab8c78-04c8-415b-9fc3-3ff901e29322"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device Number: 0\n",
            "  Device name: Tesla T4\n",
            "  max Blocks Per MultiProcessor: 16\n",
            "  max Threads Per MultiProcessor: 1024\n",
            "  max Threads Per Block: 1024\n",
            "  num SM: 40\n",
            "  num bytes sharedMem Per Block: 49152\n",
            "  num bytes sharedMem Per Multiprocessor: 65536\n",
            "  Memory Clock Rate (KHz): 5001000\n",
            "  Memory Bus Width (bits): 256\n",
            "  Peak Memory Bandwidth (GB/s): 320.064000\n",
            "\n",
            "\n",
            "Running for MATRIX_SIZE = 32 and MASK_WIDTH = 3\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.233472 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.026880 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 32 and MASK_WIDTH = 7\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.020320 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.026112 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 32 and MASK_WIDTH = 15\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.028640 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.029312 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 32 and MASK_WIDTH = 31\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.067968 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.066208 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 128 and MASK_WIDTH = 3\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.014688 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.014464 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 128 and MASK_WIDTH = 7\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.020480 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.019488 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 128 and MASK_WIDTH = 15\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.039104 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.039520 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 128 and MASK_WIDTH = 31\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.120000 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.119968 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 128 and MASK_WIDTH = 63\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.450048 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.450368 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 128 and MASK_WIDTH = 127\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 2.664640 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 2.665088 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 512 and MASK_WIDTH = 3\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.036704 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.035552 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 512 and MASK_WIDTH = 7\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.092832 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.092000 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 512 and MASK_WIDTH = 15\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.346368 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.345120 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 512 and MASK_WIDTH = 31\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 1.391968 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 1.391648 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 512 and MASK_WIDTH = 63\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 5.672864 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 5.679456 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 512 and MASK_WIDTH = 127\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 34.465759 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 34.467136 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 1024 and MASK_WIDTH = 3\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.104128 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.102816 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 1024 and MASK_WIDTH = 7\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.326944 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.329312 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 1024 and MASK_WIDTH = 15\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 1.318912 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 1.320992 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 1024 and MASK_WIDTH = 31\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 5.466336 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 4.859616 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 1024 and MASK_WIDTH = 63\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 19.870527 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 19.873247 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 1024 and MASK_WIDTH = 127\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 120.952354 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 69.669983 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 2048 and MASK_WIDTH = 3\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.157696 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.159968 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 2048 and MASK_WIDTH = 7\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.473856 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.476032 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 2048 and MASK_WIDTH = 15\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 1.937248 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 1.938304 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 2048 and MASK_WIDTH = 31\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 8.050784 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 8.056992 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 2048 and MASK_WIDTH = 63\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 32.919392 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 32.738174 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 2048 and MASK_WIDTH = 127\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 199.036987 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 198.924667 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 4096 and MASK_WIDTH = 3\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 0.585504 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 0.584384 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 4096 and MASK_WIDTH = 7\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 1.836128 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 1.834592 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 4096 and MASK_WIDTH = 15\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 7.625664 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 7.623808 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 4096 and MASK_WIDTH = 31\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 31.818975 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 31.811424 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 4096 and MASK_WIDTH = 63\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 130.644348 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 130.669113 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 4096 and MASK_WIDTH = 127\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 795.559509 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 795.426697 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 8192 and MASK_WIDTH = 3\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 2.310112 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 2.310048 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 8192 and MASK_WIDTH = 7\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 7.292928 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 7.294368 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 8192 and MASK_WIDTH = 15\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 30.444639 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 30.439457 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 8192 and MASK_WIDTH = 31\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 127.161758 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 127.185600 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 8192 and MASK_WIDTH = 63\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 523.552917 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 522.894653 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 8192 and MASK_WIDTH = 127\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 3181.115967 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 3180.700195 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 10000 and MASK_WIDTH = 3\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 3.445952 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 3.437632 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 10000 and MASK_WIDTH = 7\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 10.827776 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 10.819424 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 10000 and MASK_WIDTH = 15\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 45.424992 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 45.764832 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 10000 and MASK_WIDTH = 31\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 190.910431 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 189.395233 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 10000 and MASK_WIDTH = 63\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 779.415100 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 779.413391 ms\n",
            "\n",
            "Running for MATRIX_SIZE = 10000 and MASK_WIDTH = 127\n",
            "\n",
            "convolution_2D_shared_no_tiling Time elapsed: 4753.919434 ms\n",
            "\n",
            "convolution_2D_shared_with_tiling Time elapsed: 4759.828125 ms\n",
            "\n"
          ]
        }
      ]
    }
  ]
}